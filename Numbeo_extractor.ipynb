{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c601d1fe-29f5-4243-8993-f213c5044733",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "\n",
    "# import time\n",
    "\n",
    "# import math\n",
    "\n",
    "# import requests\n",
    "\n",
    "# import pandas as pd\n",
    "\n",
    "# from bs4 import BeautifulSoup\n",
    "\n",
    "# from pathlib import Path\n",
    " \n",
    "# # ==============================\n",
    "\n",
    "# # Config (GHS output)\n",
    "\n",
    "# # ==============================\n",
    "\n",
    "# NUMBEO_GHANA_URL_GHS = (\n",
    "\n",
    "#     \"https://www.numbeo.com/cost-of-living/country_result.jsp\"\n",
    "\n",
    "#     \"?country=Ghana&displayCurrency=GHS\"\n",
    "\n",
    "# )\n",
    "\n",
    "# WAYBACK_AVAILABLE = \"https://archive.org/wayback/available\"\n",
    " \n",
    "# # Use mid-month targets for July to improve odds of a nearby snapshot\n",
    "\n",
    "# TARGETS = {\n",
    "#     \"2023\":\"20240715\",\n",
    "\n",
    "#     \"2024\": \"20240715\",\n",
    "\n",
    "#     \"2025\": \"20250715\",\n",
    "\n",
    "# }\n",
    " \n",
    "# OUT_XLSX = \"numbeo_ghana_markets_cpi_GHS_2024_2025.xlsx\"\n",
    "\n",
    "# HEADERS = {\"User-Agent\": \"Academic-Price-Research/1.0 (email@example.com)\"}\n",
    "\n",
    "# Path(\"wayback_html\").mkdir(exist_ok=True)\n",
    " \n",
    "# # ==============================\n",
    "\n",
    "# # Wayback helpers\n",
    "\n",
    "# # ==============================\n",
    "\n",
    "# def closest_wayback(url: str, timestamp: str) -> str | None:\n",
    "\n",
    "#     r = requests.get(\n",
    "\n",
    "#         WAYBACK_AVAILABLE,\n",
    "\n",
    "#         params={\"url\": url, \"timestamp\": timestamp},\n",
    "\n",
    "#         headers=HEADERS,\n",
    "\n",
    "#         timeout=30,\n",
    "\n",
    "#     )\n",
    "\n",
    "#     r.raise_for_status()\n",
    "\n",
    "#     data = r.json()\n",
    "\n",
    "#     snap = data.get(\"archived_snapshots\", {}).get(\"closest\")\n",
    "\n",
    "#     return snap.get(\"url\") if snap and snap.get(\"available\") else None\n",
    " \n",
    "# def fetch_html(url: str) -> str | None:\n",
    "\n",
    "#     try:\n",
    "\n",
    "#         r = requests.get(url, headers=HEADERS, timeout=45)\n",
    "\n",
    "#         r.raise_for_status()\n",
    "\n",
    "#         return r.text\n",
    "\n",
    "#     except Exception:\n",
    "\n",
    "#         return None\n",
    " \n",
    "# # ==============================\n",
    "\n",
    "# # Find & parse the \"Markets\" table robustly\n",
    "\n",
    "# # ==============================\n",
    "\n",
    "# MARKET_KEYWORDS = [\n",
    "\n",
    "#     \"Milk\", \"Bread\", \"Rice\", \"Eggs\", \"Cheese\", \"Chicken\", \"Beef\",\n",
    "\n",
    "#     \"Apples\", \"Banana\", \"Oranges\", \"Tomato\", \"Potato\", \"Onion\"\n",
    "\n",
    "# ]\n",
    " \n",
    "# def find_markets_table(soup: BeautifulSoup):\n",
    "\n",
    "#     # 1) Try header text that contains \"market\" (case-insensitive)\n",
    "\n",
    "#     for h in soup.find_all([\"h1\",\"h2\",\"h3\",\"h4\"]):\n",
    "\n",
    "#         if \"market\" in h.get_text(strip=True).lower():\n",
    "\n",
    "#             tbl = h.find_next(\"table\")\n",
    "\n",
    "#             if tbl:\n",
    "\n",
    "#                 return tbl\n",
    "\n",
    "#     # 2) Fallback: scan all tables; pick the one whose first column contains many market-like items\n",
    "\n",
    "#     candidate, best_hits = None, -1\n",
    "\n",
    "#     for tbl in soup.find_all(\"table\"):\n",
    "\n",
    "#         hits = 0\n",
    "\n",
    "#         for tr in tbl.find_all(\"tr\"):\n",
    "\n",
    "#             tds = tr.find_all([\"td\",\"th\"])\n",
    "\n",
    "#             if not tds:\n",
    "\n",
    "#                 continue\n",
    "\n",
    "#             left = tds[0].get_text(\" \", strip=True)\n",
    "\n",
    "#             if any(k.lower() in left.lower() for k in MARKET_KEYWORDS):\n",
    "\n",
    "#                 hits += 1\n",
    "\n",
    "#         if hits > best_hits:\n",
    "\n",
    "#             best_hits = hits\n",
    "\n",
    "#             candidate = tbl\n",
    "\n",
    "#     # require some minimal hits to be confident\n",
    "\n",
    "#     return candidate if best_hits >= 5 else None\n",
    " \n",
    "# def extract_markets_table_ghs(html: str) -> list[dict]:\n",
    "\n",
    "#     \"\"\"\n",
    "\n",
    "#     Parse the 'Markets' table in page order.\n",
    "\n",
    "#     Returns list of dicts with: item, price_text (GHS line), price_value_ghs (float), range_text (if present).\n",
    "\n",
    "#     \"\"\"\n",
    "\n",
    "#     soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "#     table = find_markets_table(soup)\n",
    "\n",
    "#     if not table:\n",
    "\n",
    "#         return []\n",
    " \n",
    "#     out = []\n",
    "\n",
    "#     for tr in table.find_all(\"tr\"):\n",
    "\n",
    "#         tds = tr.find_all([\"td\",\"th\"])\n",
    "\n",
    "#         if len(tds) < 2:\n",
    "\n",
    "#             continue\n",
    " \n",
    "#         item = tds[0].get_text(\" \", strip=True)\n",
    "\n",
    "#         right = tds[1].get_text(\" \", strip=True)\n",
    " \n",
    "#         # Heuristic: skip subheaders or empty rows\n",
    "\n",
    "#         if not item or item.lower() in {\"markets\"}:\n",
    "\n",
    "#             continue\n",
    " \n",
    "#         # FIRST numeric token in the right cell is the displayed average (assumed in GHS)\n",
    "\n",
    "#         m = re.search(r\"([0-9]+(?:[.,][0-9]+)*)\", right)\n",
    "\n",
    "#         price_val = None\n",
    "\n",
    "#         if m:\n",
    "\n",
    "#             try:\n",
    "\n",
    "#                 price_val = float(m.group(1).replace(\",\", \"\"))\n",
    "\n",
    "#             except ValueError:\n",
    "\n",
    "#                 price_val = None\n",
    " \n",
    "#         # Optional min-max range\n",
    "\n",
    "#         rmatch = re.search(r\"([0-9][0-9,\\.]*)\\s*-\\s*([0-9][0-9,\\.]*)\", right)\n",
    "\n",
    "#         range_text = None\n",
    "\n",
    "#         if rmatch:\n",
    "\n",
    "#             range_text = f\"{rmatch.group(1)} - {rmatch.group(2)}\"\n",
    " \n",
    "#         out.append(\n",
    "\n",
    "#             {\n",
    "\n",
    "#                 \"item\": item,\n",
    "\n",
    "#                 \"price_text\": right,\n",
    "\n",
    "#                 \"price_value_ghs\": price_val,\n",
    "\n",
    "#                 \"range_text\": range_text,\n",
    "\n",
    "#             }\n",
    "\n",
    "#         )\n",
    "\n",
    "#     # keep only rows with any numeric value to avoid headers\n",
    "\n",
    "#     return [r for r in out if r[\"price_value_ghs\"] is not None]\n",
    " \n",
    "# # ==============================\n",
    "\n",
    "# # Main: fetch snapshots, parse, CPI, Excel\n",
    "\n",
    "# # ==============================\n",
    "\n",
    "# records = []\n",
    "\n",
    "# snap_meta = {}\n",
    "\n",
    "# for year, ymd in TARGETS.items():\n",
    "\n",
    "#     archived_url = closest_wayback(NUMBEO_GHANA_URL_GHS, ymd)\n",
    "\n",
    "#     snap_meta[year] = archived_url\n",
    "\n",
    "#     if not archived_url:\n",
    "\n",
    "#         print(f\"[WARN] No Wayback snapshot found for {year} at {NUMBEO_GHANA_URL_GHS}\")\n",
    "\n",
    "#         continue\n",
    " \n",
    "#     html = fetch_html(archived_url)\n",
    "\n",
    "#     if not html:\n",
    "\n",
    "#         print(f\"[WARN] Could not download archived HTML for {year}: {archived_url}\")\n",
    "\n",
    "#         continue\n",
    " \n",
    "#     # Save HTML for inspection\n",
    "\n",
    "#     Path(f\"wayback_html/snapshot_{year}.html\").write_text(html, encoding=\"utf-8\")\n",
    " \n",
    "#     rows = extract_markets_table_ghs(html)\n",
    "\n",
    "#     if not rows:\n",
    "\n",
    "#         print(f\"[WARN] Parsed 0 'Markets' rows for {year}. Inspect: wayback_html/snapshot_{year}.html\")\n",
    "\n",
    "#     else:\n",
    "\n",
    "#         print(f\"[OK] Parsed {len(rows)} 'Markets' rows for {year} from {archived_url}\")\n",
    " \n",
    "#     for idx, row in enumerate(rows, start=1):\n",
    "\n",
    "#         records.append(\n",
    "\n",
    "#             {\n",
    "\n",
    "#                 \"snapshot_year\": year,\n",
    "\n",
    "#                 \"order_on_page\": idx,  # preserve exact order\n",
    "\n",
    "#                 \"archived_url\": archived_url,\n",
    "\n",
    "#                 **row,\n",
    "\n",
    "#             }\n",
    "\n",
    "#         )\n",
    "\n",
    "#     time.sleep(1)  # be polite to the archive\n",
    " \n",
    "# df = pd.DataFrame(records)\n",
    " \n",
    "# if df.empty:\n",
    "\n",
    "#     print(\"[ERROR] No rows parsed from any snapshot; cannot compute CPI.\")\n",
    "\n",
    "#     # still write a debug workbook with just the snapshot manifest\n",
    "\n",
    "#     snap_df = pd.DataFrame([{\"year\": y, \"archived_url\": u} for y, u in snap_meta.items()])\n",
    "\n",
    "#     with pd.ExcelWriter(OUT_XLSX, engine=\"xlsxwriter\") as xl:\n",
    "\n",
    "#         snap_df.to_excel(xl, sheet_name=\"snapshots\", index=False)\n",
    "\n",
    "#     raise SystemExit(1)\n",
    " \n",
    "# # Wide format: one row per item with 2024/2025 columns in GHS\n",
    "\n",
    "# pivot = (\n",
    "\n",
    "#     df.pivot_table(\n",
    "\n",
    "#         index=[\"order_on_page\", \"item\"],\n",
    "\n",
    "#         columns=\"snapshot_year\",\n",
    "\n",
    "#         values=\"price_value_ghs\",\n",
    "\n",
    "#         aggfunc=\"first\",\n",
    "\n",
    "#     )\n",
    "\n",
    "#     .reset_index()\n",
    "\n",
    "# )\n",
    "\n",
    "# pivot.columns.name = None\n",
    "\n",
    "# pivot = pivot.sort_values(\"order_on_page\")\n",
    " \n",
    "# # Items present in BOTH snapshots (for CPI)\n",
    "\n",
    "# need_cols = [c for c in [\"2024\", \"2025\"] if c in pivot.columns]\n",
    "\n",
    "# if need_cols != [\"2024\", \"2025\"]:\n",
    "\n",
    "#     print(f\"[ERROR] Missing columns in pivot: have {pivot.columns.tolist()}\")\n",
    "\n",
    "#     # Write what we have and exit gracefully\n",
    "\n",
    "#     with pd.ExcelWriter(OUT_XLSX, engine=\"xlsxwriter\") as xl:\n",
    "\n",
    "#         df.to_excel(xl, sheet_name=\"markets_long_raw\", index=False)\n",
    "\n",
    "#         pivot.to_excel(xl, sheet_name=\"markets_wide_ghs\", index=False)\n",
    "\n",
    "#         pd.DataFrame([{\"year\": y, \"archived_url\": u} for y, u in snap_meta.items()]).to_excel(\n",
    "\n",
    "#             xl, sheet_name=\"snapshots\", index=False\n",
    "\n",
    "#         )\n",
    "\n",
    "#     raise SystemExit(1)\n",
    " \n",
    "# both = pivot.dropna(subset=[\"2024\", \"2025\"]).copy()\n",
    " \n",
    "# # Equal-weights Laspeyres CPI: 2024 = 100\n",
    "\n",
    "# if not both.empty:\n",
    "\n",
    "#     base_sum = both[\"2024\"].sum()\n",
    "\n",
    "#     curr_sum = both[\"2025\"].sum()\n",
    "\n",
    "#     cpi_2024 = 100.0\n",
    "\n",
    "#     cpi_2025 = (curr_sum / base_sum) * 100.0 if base_sum > 0 else math.nan\n",
    "\n",
    "#     both[\"price_relative_%\"] = (both[\"2025\"] / both[\"2024\"] - 1.0) * 100.0\n",
    "\n",
    "# else:\n",
    "\n",
    "#     base_sum = curr_sum = cpi_2024 = cpi_2025 = math.nan\n",
    "\n",
    "#     print(\"[WARN] No overlapping items between 2024 and 2025; CPI cannot be computed.\")\n",
    " \n",
    "# # Snapshot manifest\n",
    "\n",
    "# snapshots = pd.DataFrame([{\"year\": y, \"archived_url\": u} for y, u in snap_meta.items()])\n",
    " \n",
    "# # Write Excel\n",
    "\n",
    "# with pd.ExcelWriter(OUT_XLSX, engine=\"xlsxwriter\") as xl:\n",
    "\n",
    "#     df.to_excel(xl, sheet_name=\"markets_long_raw\", index=False)\n",
    "\n",
    "#     pivot.to_excel(xl, sheet_name=\"markets_wide_ghs\", index=False)\n",
    "\n",
    "#     if not both.empty:\n",
    "\n",
    "#         both.to_excel(xl, sheet_name=\"matched_items\", index=False)\n",
    "\n",
    "#         pd.DataFrame(\n",
    "\n",
    "#             {\n",
    "\n",
    "#                 \"metric\": [\"base_sum_2024_GHS\", \"curr_sum_2025_GHS\", \"CPI_2024=100\", \"CPI_2025\"],\n",
    "\n",
    "#                 \"value\": [base_sum, curr_sum, 100.0, cpi_2025],\n",
    "\n",
    "#             }\n",
    "\n",
    "#         ).to_excel(xl, sheet_name=\"summary\", index=False)\n",
    "\n",
    "#     snapshots.to_excel(xl, sheet_name=\"snapshots\", index=False)\n",
    " \n",
    "# print(f\"Done. Wrote {OUT_XLSX}\")\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0fef76e0-5174-4122-90af-db43a3fcfcc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xlsxwriter\n",
      "  Downloading xlsxwriter-3.2.5-py3-none-any.whl.metadata (2.7 kB)\n",
      "Downloading xlsxwriter-3.2.5-py3-none-any.whl (172 kB)\n",
      "Installing collected packages: xlsxwriter\n",
      "Successfully installed xlsxwriter-3.2.5\n"
     ]
    }
   ],
   "source": [
    "!pip install xlsxwriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9261e9a7-25a3-4081-a684-434eba2d7cd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using snapshot 20230715 for 2023: http://web.archive.org/web/20210614115541/https://www.numbeo.com/cost-of-living/country_result.jsp?country=Ghana&displayCurrency=GHS\n",
      "[OK] Parsed 55 'Markets' rows for 2023 from http://web.archive.org/web/20210614115541/https://www.numbeo.com/cost-of-living/country_result.jsp?country=Ghana&displayCurrency=GHS\n",
      "[INFO] Using snapshot 20240715 for 2024: http://web.archive.org/web/20210614115541/https://www.numbeo.com/cost-of-living/country_result.jsp?country=Ghana&displayCurrency=GHS\n",
      "[OK] Parsed 55 'Markets' rows for 2024 from http://web.archive.org/web/20210614115541/https://www.numbeo.com/cost-of-living/country_result.jsp?country=Ghana&displayCurrency=GHS\n",
      "[INFO] Using snapshot 20250801 for 2025: http://web.archive.org/web/20210614115541/https://www.numbeo.com/cost-of-living/country_result.jsp?country=Ghana&displayCurrency=GHS\n",
      "[OK] Parsed 55 'Markets' rows for 2025 from http://web.archive.org/web/20210614115541/https://www.numbeo.com/cost-of-living/country_result.jsp?country=Ghana&displayCurrency=GHS\n",
      "✅ Done. Wrote numbeo_ghana_markets_cpi_GHS_2024_2025.xlsx\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import time\n",
    "import math\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from pathlib import Path\n",
    "\n",
    "# ==============================\n",
    "# Config (GHS output)\n",
    "# ==============================\n",
    "\n",
    "NUMBEO_GHANA_URL_GHS = (\n",
    "    \"https://www.numbeo.com/cost-of-living/country_result.jsp\"\n",
    "    \"?country=Ghana&displayCurrency=GHS\"\n",
    ")\n",
    "WAYBACK_AVAILABLE = \"https://archive.org/wayback/available\"\n",
    "\n",
    "# Use mid-month targets, plus fallbacks\n",
    "TARGETS = {\n",
    "    \"2023\": [\"20230715\", \"20230801\", \"20230615\"],\n",
    "    \"2024\": [\"20240715\", \"20240801\", \"20240615\"],\n",
    "    \"2025\": [\"20250715\", \"20250801\", \"20250615\"],\n",
    "}\n",
    "\n",
    "OUT_XLSX = \"numbeo_ghana_markets_cpi_GHS_2024_2025.xlsx\"\n",
    "HEADERS = {\"User-Agent\": \"Academic-Price-Research/1.0 (email@example.com)\"}\n",
    "\n",
    "Path(\"wayback_html\").mkdir(exist_ok=True)\n",
    "\n",
    "# ==============================\n",
    "# Wayback helpers\n",
    "# ==============================\n",
    "\n",
    "def closest_wayback(url: str, timestamp: str) -> str | None:\n",
    "    r = requests.get(\n",
    "        WAYBACK_AVAILABLE,\n",
    "        params={\"url\": url, \"timestamp\": timestamp},\n",
    "        headers=HEADERS,\n",
    "        timeout=30,\n",
    "    )\n",
    "    r.raise_for_status()\n",
    "    data = r.json()\n",
    "    snap = data.get(\"archived_snapshots\", {}).get(\"closest\")\n",
    "    return snap.get(\"url\") if snap and snap.get(\"available\") else None\n",
    "\n",
    "def fetch_html(url: str) -> str | None:\n",
    "    try:\n",
    "        r = requests.get(url, headers=HEADERS, timeout=45)\n",
    "        r.raise_for_status()\n",
    "        return r.text\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def find_best_snapshot(url: str, year: str, timestamps: list) -> str | None:\n",
    "    for ts in timestamps:\n",
    "        snap = closest_wayback(url, ts)\n",
    "        if snap:\n",
    "            print(f\"[INFO] Using snapshot {ts} for {year}: {snap}\")\n",
    "            return snap\n",
    "    print(f\"[WARN] No snapshot found for {year} using any fallback dates.\")\n",
    "    return None\n",
    "\n",
    "# ==============================\n",
    "# Find & parse the \"Markets\" table\n",
    "# ==============================\n",
    "\n",
    "MARKET_KEYWORDS = [\n",
    "    \"Milk\", \"Bread\", \"Rice\", \"Eggs\", \"Cheese\", \"Chicken\", \"Beef\",\n",
    "    \"Apples\", \"Banana\", \"Oranges\", \"Tomato\", \"Potato\", \"Onion\"\n",
    "]\n",
    "\n",
    "def find_markets_table(soup: BeautifulSoup):\n",
    "    for h in soup.find_all([\"h1\", \"h2\", \"h3\", \"h4\"]):\n",
    "        if \"market\" in h.get_text(strip=True).lower():\n",
    "            tbl = h.find_next(\"table\")\n",
    "            if tbl:\n",
    "                return tbl\n",
    "    candidate, best_hits = None, -1\n",
    "    for tbl in soup.find_all(\"table\"):\n",
    "        hits = 0\n",
    "        for tr in tbl.find_all(\"tr\"):\n",
    "            tds = tr.find_all([\"td\", \"th\"])\n",
    "            if not tds:\n",
    "                continue\n",
    "            left = tds[0].get_text(\" \", strip=True)\n",
    "            if any(k.lower() in left.lower() for k in MARKET_KEYWORDS):\n",
    "                hits += 1\n",
    "        if hits > best_hits:\n",
    "            best_hits = hits\n",
    "            candidate = tbl\n",
    "    return candidate if best_hits >= 5 else None\n",
    "\n",
    "def extract_markets_table_ghs(html: str) -> list[dict]:\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    table = find_markets_table(soup)\n",
    "    if not table:\n",
    "        return []\n",
    "    out = []\n",
    "    for tr in table.find_all(\"tr\"):\n",
    "        tds = tr.find_all([\"td\", \"th\"])\n",
    "        if len(tds) < 2:\n",
    "            continue\n",
    "        item = tds[0].get_text(\" \", strip=True)\n",
    "        right = tds[1].get_text(\" \", strip=True)\n",
    "        if not item or item.lower() in {\"markets\"}:\n",
    "            continue\n",
    "        m = re.search(r\"([0-9]+(?:[.,][0-9]+)*)\", right)\n",
    "        price_val = None\n",
    "        if m:\n",
    "            try:\n",
    "                price_val = float(m.group(1).replace(\",\", \"\"))\n",
    "            except ValueError:\n",
    "                price_val = None\n",
    "        rmatch = re.search(r\"([0-9][0-9,\\.]*)\\s*-\\s*([0-9][0-9,\\.]*)\", right)\n",
    "        range_text = None\n",
    "        if rmatch:\n",
    "            range_text = f\"{rmatch.group(1)} - {rmatch.group(2)}\"\n",
    "        out.append({\n",
    "            \"item\": item,\n",
    "            \"price_text\": right,\n",
    "            \"price_value_ghs\": price_val,\n",
    "            \"range_text\": range_text,\n",
    "        })\n",
    "    return [r for r in out if r[\"price_value_ghs\"] is not None]\n",
    "\n",
    "# ==============================\n",
    "# Main Process\n",
    "# ==============================\n",
    "\n",
    "records = []\n",
    "snap_meta = {}\n",
    "\n",
    "for year, fallback_dates in TARGETS.items():\n",
    "    archived_url = find_best_snapshot(NUMBEO_GHANA_URL_GHS, year, fallback_dates)\n",
    "    snap_meta[year] = archived_url\n",
    "    if not archived_url:\n",
    "        continue\n",
    "\n",
    "    html = fetch_html(archived_url)\n",
    "    if not html:\n",
    "        print(f\"[WARN] Could not download archived HTML for {year}: {archived_url}\")\n",
    "        continue\n",
    "\n",
    "    Path(f\"wayback_html/snapshot_{year}.html\").write_text(html, encoding=\"utf-8\")\n",
    "\n",
    "    rows = extract_markets_table_ghs(html)\n",
    "    if not rows:\n",
    "        print(f\"[WARN] Parsed 0 'Markets' rows for {year}. Inspect: wayback_html/snapshot_{year}.html\")\n",
    "    else:\n",
    "        print(f\"[OK] Parsed {len(rows)} 'Markets' rows for {year} from {archived_url}\")\n",
    "\n",
    "    for idx, row in enumerate(rows, start=1):\n",
    "        records.append({\n",
    "            \"snapshot_year\": year,\n",
    "            \"order_on_page\": idx,\n",
    "            \"archived_url\": archived_url,\n",
    "            **row,\n",
    "        })\n",
    "\n",
    "    time.sleep(1)\n",
    "\n",
    "df = pd.DataFrame(records)\n",
    "\n",
    "# Snapshot manifest\n",
    "snapshots = pd.DataFrame([{\"year\": y, \"archived_url\": u} for y, u in snap_meta.items()])\n",
    "\n",
    "# Write partial if empty\n",
    "if df.empty:\n",
    "    print(\"[ERROR] No data parsed. Exiting.\")\n",
    "    with pd.ExcelWriter(OUT_XLSX, engine=\"xlsxwriter\") as xl:\n",
    "        snapshots.to_excel(xl, sheet_name=\"snapshots\", index=False)\n",
    "    raise SystemExit(1)\n",
    "\n",
    "# Pivot wide format\n",
    "pivot = (\n",
    "    df.pivot_table(\n",
    "        index=[\"order_on_page\", \"item\"],\n",
    "        columns=\"snapshot_year\",\n",
    "        values=\"price_value_ghs\",\n",
    "        aggfunc=\"first\",\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "pivot.columns.name = None\n",
    "pivot = pivot.sort_values(\"order_on_page\")\n",
    "\n",
    "# CPI if both years are available\n",
    "need_cols = [c for c in [\"2024\", \"2025\"] if c in pivot.columns]\n",
    "cpi_df = None\n",
    "summary_df = None\n",
    "\n",
    "if set(need_cols) == {\"2024\", \"2025\"}:\n",
    "    both = pivot.dropna(subset=[\"2024\", \"2025\"]).copy()\n",
    "    if not both.empty:\n",
    "        base_sum = both[\"2024\"].sum()\n",
    "        curr_sum = both[\"2025\"].sum()\n",
    "        cpi_2024 = 100.0\n",
    "        cpi_2025 = (curr_sum / base_sum) * 100.0 if base_sum > 0 else math.nan\n",
    "        both[\"price_relative_%\"] = (both[\"2025\"] / both[\"2024\"] - 1.0) * 100.0\n",
    "        cpi_df = both\n",
    "        summary_df = pd.DataFrame({\n",
    "            \"metric\": [\"base_sum_2024_GHS\", \"curr_sum_2025_GHS\", \"CPI_2024=100\", \"CPI_2025\"],\n",
    "            \"value\": [base_sum, curr_sum, 100.0, cpi_2025],\n",
    "        })\n",
    "    else:\n",
    "        print(\"[WARN] No overlapping items between 2024 and 2025; CPI not computed.\")\n",
    "\n",
    "# Write Excel\n",
    "with pd.ExcelWriter(OUT_XLSX, engine=\"xlsxwriter\") as xl:\n",
    "    df.to_excel(xl, sheet_name=\"markets_long_raw\", index=False)\n",
    "    pivot.to_excel(xl, sheet_name=\"markets_wide_ghs\", index=False)\n",
    "    snapshots.to_excel(xl, sheet_name=\"snapshots\", index=False)\n",
    "    if cpi_df is not None:\n",
    "        cpi_df.to_excel(xl, sheet_name=\"matched_items\", index=False)\n",
    "    if summary_df is not None:\n",
    "        summary_df.to_excel(xl, sheet_name=\"summary\", index=False)\n",
    "\n",
    "print(f\"✅ Done. Wrote {OUT_XLSX}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb467ec7-82f6-4b9c-87c4-c4e98a47301a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
